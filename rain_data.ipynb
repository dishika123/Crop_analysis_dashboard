{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "134027e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated in-place:\n",
      "Total rows: 2941\n",
      "Unique SD_Name count: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "file_path = \"C:\\\\Users\\\\dishi\\\\Downloads\\\\crop\\\\Area_Weighted_Monthly_Seasonal_And_Annual_Rainfall_0.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define how combined SD_Name entries should be split\n",
    "split_mapping = {\n",
    "    \"ASSAM & MEGHALAYA\": [\"ASSAM\", \"MEGHALAYA\"],\n",
    "    \"HARYANA, DELHI & CHANDIGARH\": [\"HARYANA\", \"DELHI\", \"CHANDIGARH\"],\n",
    "    \"NAGALAND, MANIPUR, MIZORAM,TRIPURA\": [\"NAGALAND\", \"MANIPUR\", \"MIZORAM\", \"TRIPURA\"],\n",
    "    \"SUB-HIMALAYAN W BENGAL & SIKKIM\": [\"WEST BENGAL\", \"SIKKIM\"],\n",
    "    \"TAMIL NADU & PONDICHERRY\": [\"TAMIL NADU\", \"PUDUCHERRY\"],\n",
    "    \"GUJARAT REGION, DADRA & NAGAR HAVELI\": [\"GUJARAT\", \"DADRA & NAGAR HAVELI\"],\n",
    "    \"SAURASHTRA KUTCH & DIU\": [\"GUJARAT\", \"DIU\"]\n",
    "}\n",
    "\n",
    "# Expand rows in-place and update df directly\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    sd_value = str(row[\"SD_Name\"]).strip().upper()\n",
    "    if sd_value in split_mapping:\n",
    "        for new_sd in split_mapping[sd_value]:\n",
    "            new_row = row.copy()\n",
    "            new_row[\"SD_Name\"] = new_sd\n",
    "            expanded_rows.append(new_row)\n",
    "    else:\n",
    "        expanded_rows.append(row)\n",
    "\n",
    "# Replace df with the expanded data\n",
    "df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Check result summary\n",
    "print(\"✅ Updated in-place:\")\n",
    "print(\"Total rows:\", df.shape[0])\n",
    "print(\"Unique SD_Name count:\", df['SD_Name'].nunique())\n",
    "\n",
    "# (Optional) If you want to save changes:\n",
    "# df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868268ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame updated: East/West subdivisions merged into unified states.\n",
      "Unique SD_Name values now: ['ANDAMAN & NICOBAR ISLANDS', 'ARUNACHAL PRADESH', 'ASSAM', 'BIHAR', 'CHANDIGARH', 'CHATTISGARH', 'COASTAL KARNATAKA', 'COSTAL ANDHRA PRADESH', 'DADRA & NAGAR HAVELI', 'DELHI', 'DIU', 'GANGETIC WEST BENGAL', 'GUJARAT', 'HARYANA', 'HIMACHAL PRADESH', 'JAMMU & KASHMIR', 'JHARKHAND', 'KERALA', 'KOKAN & GOA', 'LAKSHADWEEP', 'MADHYA MAHARASHTRA', 'MADHYA PRADESH', 'MANIPUR', 'MARATWADA', 'MEGHALAYA', 'MIZORAM', 'NAGALAND', 'NORTH INTERIOR KARNATAKA', 'ORISSA', 'PUDUCHERRY', 'PUNJAB', 'RAJASTHAN', 'RAYALSEEMA', 'SIKKIM', 'SOUTH INTERIOR KARNATAKA', 'TAMIL NADU', 'TELENGANA', 'TRIPURA', 'UTTAR PRADESH', 'UTTARANCHAL', 'VIDARBHA', 'WEST BENGAL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_19164\\4033221041.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2️⃣ Clean SD_Name formatting ---\n",
    "df[\"SD_Name\"] = (\n",
    "    df[\"SD_Name\"]\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# --- 3️⃣ Define east-west merge map with weights ---\n",
    "merge_map = {\n",
    "    \"EAST RAJASTHAN\": (\"RAJASTHAN\", 0.7),\n",
    "    \"WEST RAJASTHAN\": (\"RAJASTHAN\", 0.3),\n",
    "    \"EAST MADHYA PRADESH\": (\"MADHYA PRADESH\", 0.6),\n",
    "    \"WEST MADHYA PRADESH\": (\"MADHYA PRADESH\", 0.4),\n",
    "    \"EAST UTTAR PRADESH\": (\"UTTAR PRADESH\", 0.6),\n",
    "    \"WEST UTTAR PRADESH\": (\"UTTAR PRADESH\", 0.4),\n",
    "}\n",
    "\n",
    "# --- 4️⃣ Map unified state names + weights ---\n",
    "df[\"STATE\"] = df[\"SD_Name\"].apply(lambda x: merge_map.get(x, (x, 1))[0])\n",
    "df[\"WEIGHT\"] = df[\"SD_Name\"].apply(lambda x: merge_map.get(x, (x, 1))[1])\n",
    "\n",
    "# --- 5️⃣ Perform weighted aggregation (keep numeric columns only) ---\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.difference([\"WEIGHT\"])\n",
    "grouped = (\n",
    "    df.groupby([\"YEAR\", \"STATE\"], as_index=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        col: (g[col] * g[\"WEIGHT\"]).sum() / g[\"WEIGHT\"].sum() if col in numeric_cols else None\n",
    "        for col in numeric_cols\n",
    "    }))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- 6️⃣ Replace old DataFrame with merged version ---\n",
    "df = grouped.copy()\n",
    "df.rename(columns={\"STATE\": \"SD_Name\"}, inplace=True)\n",
    "\n",
    "# ✅ In-memory update complete\n",
    "print(\"✅ DataFrame updated: East/West subdivisions merged into unified states.\")\n",
    "print(\"Unique SD_Name values now:\", sorted(df[\"SD_Name\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3fe24c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All subdivisions mapped successfully.\n",
      "✅ Duplicates merged — weighted mean computed for repeating Year+State pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_19164\\884629223.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Step 1: Define the IMD → State mapping ---\n",
    "imd_to_state = {\n",
    "    'ANDAMAN & NICOBAR ISLANDS': 'Andaman and Nicobar Islands',\n",
    "    'ARUNACHAL PRADESH': 'Arunachal Pradesh',\n",
    "    'ASSAM': 'Assam',\n",
    "    'MEGHALAYA': 'Meghalaya',\n",
    "    'NAGALAND': 'Nagaland',\n",
    "    'MANIPUR': 'Manipur',\n",
    "    'MIZORAM': 'Mizoram',\n",
    "    'TRIPURA': 'Tripura',\n",
    "    'GANGETIC WEST BENGAL': 'West Bengal',\n",
    "    'WEST BENGAL': 'West Bengal',\n",
    "    'SIKKIM': 'Sikkim',\n",
    "    'ORISSA': 'Odisha',\n",
    "    'JHARKHAND': 'Jharkhand',\n",
    "    'BIHAR': 'Bihar',\n",
    "    'UTTAR PRADESH': 'Uttar Pradesh',\n",
    "    'UTTARANCHAL': 'Uttarakhand',\n",
    "    'HARYANA': 'Haryana',\n",
    "    'CHANDIGARH': 'Chandigarh',\n",
    "    'PUNJAB': 'Punjab',\n",
    "    'HIMACHAL PRADESH': 'Himachal Pradesh',\n",
    "    'JAMMU & KASHMIR': 'Jammu and Kashmir',\n",
    "    'DELHI': 'Delhi',\n",
    "    'RAJASTHAN': 'Rajasthan',\n",
    "    'GUJARAT': 'Gujarat',\n",
    "    'KOKAN & GOA': 'Goa',\n",
    "    'MADHYA MAHARASHTRA': 'Maharashtra',\n",
    "    'MARATWADA': 'Maharashtra',\n",
    "    'VIDARBHA': 'Maharashtra',\n",
    "    'CHATTISGARH': 'Chhattisgarh',\n",
    "    'MADHYA PRADESH': 'Madhya Pradesh',\n",
    "    'COASTAL KARNATAKA': 'Karnataka',\n",
    "    'NORTH INTERIOR KARNATAKA': 'Karnataka',\n",
    "    'SOUTH INTERIOR KARNATAKA': 'Karnataka',\n",
    "    'TELENGANA': 'Telangana',\n",
    "    'RAYALSEEMA': 'Andhra Pradesh',\n",
    "    'COSTAL ANDHRA PRADESH': 'Andhra Pradesh',\n",
    "    'TAMIL NADU': 'Tamil Nadu',\n",
    "    'KERALA': 'Kerala',\n",
    "    'LAKSHADWEEP': 'Lakshadweep',\n",
    "    'DADRA & NAGAR HAVELI': 'Dadra and Nagar Haveli',\n",
    "    'DIU': 'Daman and Diu',\n",
    "    'PUDUCHERRY': 'Puducherry'\n",
    "}\n",
    "\n",
    "# --- Step 2: Map IMD Subdivision to State ---\n",
    "df[\"State\"] = df[\"SD_Name\"].map(imd_to_state)\n",
    "\n",
    "# --- Step 3: Check unmapped values ---\n",
    "unmapped = df[df[\"State\"].isna()][\"SD_Name\"].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(\"⚠️ Unmapped Subdivisions:\", unmapped)\n",
    "else:\n",
    "    print(\"✅ All subdivisions mapped successfully.\")\n",
    "\n",
    "# --- Step 4: Weighted mean aggregation ---\n",
    "# Assuming you have columns: 'Year', 'Rainfall', and 'Weight' (e.g. area or population)\n",
    "# If you don’t have a 'Weight' column, create uniform weights = 1\n",
    "if 'Weight' not in df.columns:\n",
    "    df[\"Weight\"] = 1\n",
    "\n",
    "# Weighted mean formula: sum(value * weight) / sum(weight)\n",
    "df_grouped = (\n",
    "    df.groupby([\"YEAR\", \"State\"], as_index=False)\n",
    "      .apply(lambda g: pd.Series({\n",
    "          **{col: (g[col] * g[\"Weight\"]).sum() / g[\"Weight\"].sum()\n",
    "             for col in g.columns if col not in [\"Year\", \"State\", \"SD_Name\", \"Weight\"] and pd.api.types.is_numeric_dtype(g[col])},\n",
    "          \"Weight\": g[\"Weight\"].sum()\n",
    "      }))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Step 5: Replace df with the new grouped version ---\n",
    "df = df_grouped\n",
    "\n",
    "print(\"✅ Duplicates merged — weighted mean computed for repeating Year+State pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c188a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique State values (36):\n",
      "================================================================================\n",
      "  - Andaman and Nicobar Islands\n",
      "  - Andhra Pradesh\n",
      "  - Arunachal Pradesh\n",
      "  - Assam\n",
      "  - Bihar\n",
      "  - Chandigarh\n",
      "  - Chhattisgarh\n",
      "  - Dadra and Nagar Haveli\n",
      "  - Daman and Diu\n",
      "  - Delhi\n",
      "  - Goa\n",
      "  - Gujarat\n",
      "  - Haryana\n",
      "  - Himachal Pradesh\n",
      "  - Jammu and Kashmir\n",
      "  - Jharkhand\n",
      "  - Karnataka\n",
      "  - Kerala\n",
      "  - Lakshadweep\n",
      "  - Madhya Pradesh\n",
      "  - Maharashtra\n",
      "  - Manipur\n",
      "  - Meghalaya\n",
      "  - Mizoram\n",
      "  - Nagaland\n",
      "  - Odisha\n",
      "  - Puducherry\n",
      "  - Punjab\n",
      "  - Rajasthan\n",
      "  - Sikkim\n",
      "  - Tamil Nadu\n",
      "  - Telangana\n",
      "  - Tripura\n",
      "  - Uttar Pradesh\n",
      "  - Uttarakhand\n",
      "  - West Bengal\n"
     ]
    }
   ],
   "source": [
    "# Get unique State values\n",
    "unique_state_names = df['State'].unique()\n",
    "\n",
    "print(f\"Unique State values ({len(unique_state_names)}):\")\n",
    "print(\"=\"*80)\n",
    "for name in sorted(unique_state_names):\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51858743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered dataframe: 360 rows remain (2005–2014 only)\n"
     ]
    }
   ],
   "source": [
    "# Filter data to keep only years between 2005 and 2014 (inclusive)\n",
    "df = df[(df[\"YEAR\"] >= 2005) & (df[\"YEAR\"] <= 2014)]\n",
    "\n",
    "print(f\"✅ Filtered dataframe: {len(df)} rows remain (2005–2014 only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e3321a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame successfully saved as 'rain.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export final dataframe to CSV\n",
    "df.to_csv(\"rain.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ DataFrame successfully saved as 'rain.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c9e67e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 360 entries, 1941 to 2300\n",
      "Data columns (total 19 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   State    360 non-null    object \n",
      " 1   ANNUAL   360 non-null    float64\n",
      " 2   APR      360 non-null    float64\n",
      " 3   AUG      360 non-null    float64\n",
      " 4   DEC      360 non-null    float64\n",
      " 5   FEB      360 non-null    float64\n",
      " 6   JAN      360 non-null    float64\n",
      " 7   JAN-FEB  360 non-null    float64\n",
      " 8   JUL      360 non-null    float64\n",
      " 9   JUN      360 non-null    float64\n",
      " 10  Jun-Sep  360 non-null    float64\n",
      " 11  MAR      360 non-null    float64\n",
      " 12  MAY      360 non-null    float64\n",
      " 13  Mar-May  360 non-null    float64\n",
      " 14  NOV      360 non-null    float64\n",
      " 15  OCT      360 non-null    float64\n",
      " 16  Oct-Dec  360 non-null    float64\n",
      " 17  SEP      360 non-null    float64\n",
      " 18  Year     360 non-null    int64  \n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 56.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "350536e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column renamed: 'YEAR' → 'Year'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_19164\\895440806.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={'YEAR': 'Year'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Rename YEAR column to Year\n",
    "df.rename(columns={'YEAR': 'Year'}, inplace=True)\n",
    "print(\"✅ Column renamed: 'YEAR' → 'Year'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "418e37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Year column converted to int\n",
      "\n",
      "Data type: int64\n",
      "Sample values: [2005 2006 2007 2008 2009 2010 2011 2012 2013 2014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_19164\\3665913456.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Year'] = df['Year'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Convert Year column from float64 to int\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "print(\"✅ Year column converted to int\")\n",
    "print(f\"\\nData type: {df['Year'].dtype}\")\n",
    "print(f\"Sample values: {df['Year'].unique()[:10]}\")\n",
    "\n",
    "df.to_csv('rain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85e5bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before dropping:\n",
      "['State', 'ANNUAL', 'APR', 'AUG', 'DEC', 'FEB', 'JAN', 'JAN-FEB', 'JUL', 'JUN', 'Jun-Sep', 'MAR', 'MAY', 'Mar-May', 'NOV', 'OCT', 'Oct-Dec', 'SEP', 'Year']\n",
      "\n",
      "Total columns: 19\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Columns after dropping:\n",
      "['State', 'ANNUAL', 'APR', 'AUG', 'DEC', 'FEB', 'JAN', 'JUL', 'JUN', 'MAR', 'MAY', 'NOV', 'OCT', 'SEP', 'Year']\n",
      "\n",
      "Total columns: 15\n",
      "\n",
      "✅ Dropped columns: Weight, SD NO.\n",
      "✅ Updated rain.csv saved\n"
     ]
    }
   ],
   "source": [
    "# Drop Weight and SD NO. columns\n",
    "print(\"Columns before dropping:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=['Weight', 'SD NO.','Oct-Dec','Mar-May','Jun-Sep','JAN-FEB'], errors='ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nColumns after dropping:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n✅ Dropped columns: Weight, SD NO.\")\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv('rain.csv', index=False)\n",
    "print(\"✅ Updated rain.csv saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c60b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
